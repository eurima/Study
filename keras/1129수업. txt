이진분류에서는 mse 사용이 불가능 하다
ex) 동전던지기

-> 무조건 binary cross entropy
★ sigmoid
-----------
회귀
이진
다중

회귀 아니면 분류!
활성화함수 (Activation 함수)
[ Y = wx + b ] 를 감싸준다
디폴트 값은 Linear 함수
마지막 레이어에서 사용하여 0~1 사이의 값으로 만들어 준다

회귀모델의 경우는
히든레이어 부분에서 한정해준다

이진분류에서는
Activation 은 Sigmoid
Loss 는 Binay Cross Entropy 만 쓴다

============ 다중 분류 ==============

마지막 노드의 숫자는
class의 숫자와 동일 해야 한다
ex) ○, □, △ 세개를 분류 하는 모델은 => 마지막 노드의 숫자는 3


softmax 함수의 활용
softmax는 다중분류에서 activation 함수로 마지막 1회만 사용 한다
(히든 레이어 사용 금지!!)
출력 값은 [ 0.43, 0.27, 0.3 ] 등의 합이 1이 되는 확률로 표시 된다
(N, 3) 의 형태로 표시


★ One Hot Encording

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 
표현하고 싶은 단어의 인덱스에 1의 값을 부여하고,
다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식.

이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector).
원-핫 인코딩 과정 정리

(1) 각 라벨에 고유한 인덱스를 부여 (정수 인코딩)
(2) 표현하고 싶은 라벨의 인덱스의 위치에 1을 부여하고, 
다른 라벨의 인덱스의 위치에는 0을 부여

학습전 편중 되지 않게 라벨을 0,1 로 만들어서 학습

ex) 
○, □, △ => 0, 1, 2 로 분류 되어 있다면
△가 □보다 2배 중요하다는 의미로 받아들일 수 있다

○ = [1, 0, 0] 合 1
□ = [0, 1, 0] 合 1
△ = [0, 0, 1] 合 1
으로 변환하여 입력 시킴
각각 合 1 이 되어
왜곡되는 현상을 X
평등하게!!

input data 단계에서
y값을 OneHotEncording 해준다
y의 shape는 (N,) => (N,3) 으로 전환이 된다

※ 다중 분류를 만나게 되면!
1) y 값 확인 후 -> 라벨이 몇개인가?
2) 라벨 수에 맞추어 OneHotEncording

softmax
categorical_crossentropy


